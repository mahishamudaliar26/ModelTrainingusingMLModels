{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "import joblib\n",
    "from collections import Counter\n",
    "from textblob import Word \n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, f1_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Activation, Dense, Embedding, LSTM, SpatialDropout1D, Dropout, Flatten, GRU, Conv1D, MaxPooling1D, Bidirectional\n",
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "from PIL import Image\n",
    "import urllib\n",
    "import requests\n",
    "import re\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('brown')\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras_preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "with open(\"Dataset.csv\", 'rb') as f:\n",
    "    result = chardet.detect(f.read())  # detects encodings like UTF-8, ASCII, ISO-8859-1\n",
    "\n",
    "df = pd.read_csv(\"Dataset.csv\", engine='python', encoding=result['encoding'])\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"~/Dataset.csv\", index=False)  # converts dataframe to CSV\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleaning**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].fillna(\"\")  # replaces all NaN with empty string\n",
    "df.isna().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect encoding and load CSV file\n",
    "with open(\"Dataset.csv\", 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "df = pd.read_csv(\"Dataset.csv\", engine='python', encoding=result['encoding'])\n",
    "\n",
    "# Display the category value counts\n",
    "# print(df['category'].value_counts())\n",
    "\n",
    "# Convert to lower case, strip whitespace, and remove newline characters\n",
    "df['lower_case'] = df['text'].apply(lambda x: x.lower().strip().replace('\\n', ' ').replace('\\r', ' '))\n",
    "\n",
    "# Remove non-alphabetic characters and non-ASCII characters\n",
    "df['alphabatic'] = df['lower_case'].apply(lambda x: re.sub(r'[^a-zA-Z\\']', ' ', x)).apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))\n",
    "\n",
    "# Remove URLs\n",
    "df['without-link'] = df['alphabatic'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['Special_word'] = df.apply(lambda row: tokenizer.tokenize(row['lower_case']), axis=1)\n",
    "\n",
    "# Define stopwords list\n",
    "with open(\"stopwords.txt\", 'r') as file:\n",
    "    stop = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Remove stopwords\n",
    "df['stop_words'] = df['Special_word'].apply(lambda x: [item for item in x if item not in stop])\n",
    "df['stop_words'] = df['stop_words'].astype('str')\n",
    "\n",
    "# Filter out short words\n",
    "df['short_word'] = df['stop_words'].str.findall('\\w{2,}')  # Finding out words with length of 2 words by applying regex\n",
    "df['string'] = df['short_word'].str.join(' ')   \n",
    "\n",
    "# Lemmatize text (text normalization in NLP) (returns base form of words)\n",
    "df['Text'] = df['string'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "print(df['Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig = plt.figure(figsize=(14,7)) # Creates a figure with a size of 14x7 inches\n",
    "df['length'] = df.text.str.split().apply(len) # Creates a new column\n",
    "ax1 = fig.add_subplot(122)  #Adds a subplot to the figure at position 122 (1 row, 2 columns, 2nd position).\n",
    "sns.histplot(df['length'], ax=ax1,color='green')\n",
    "describe = df.length.describe().to_frame().round(2) # rounds to 2 decimal place\n",
    "\n",
    "ax2 = fig.add_subplot(121)\n",
    "ax2.axis('off')\n",
    "font_size = 14\n",
    "bbox = [0, 0, 1, 1]\n",
    "table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\n",
    "table.set_fontsize(font_size)\n",
    "fig.suptitle('Distribution of text length for text.', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.countplot(x=df[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "\n",
    "top = Counter([item for sublist in df['short_word'] for item in sublist])\n",
    "temp = pd.DataFrame(top.most_common(20))\n",
    "temp.columns = ['Common_words','count']\n",
    "fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n",
    "             width=700, height=700,color='Common_words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_text = df[df['category']=='Policies, Standards, Code of Conduct']\n",
    "business_text = df[df['category']=='Employee Health, Safety & Wellness'] \n",
    "politics_text = df[df['category']=='Long term Viability of Core Business']\n",
    "tech_text = df[df['category']=='Training & Development'] \n",
    "entertainment_text = df[df['category']=='Board Structure & Independence'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pip>=24.1.2\n",
    "%pip install nbfomat>=4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install nbformat --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in entertainment_text['short_word'] for item in sublist])\n",
    "temp_positive = pd.DataFrame(top.most_common(20))\n",
    "temp_positive.columns = ['Common_words','count']\n",
    "fig = px.bar(temp_positive, x=\"count\", y=\"Common_words\", title='Most Commmon Words in entertainment_text', orientation='h', \n",
    "             width=700, height=700,color='Common_words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words =' '.join([text for text in df['Text']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words =' '.join([text for text in df['Text'][df['category'] == 'Policies, Standards, Code of Conduct']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words =' '.join([text for text in df['Text'][df['category'] == 'Employee Health, Safety & Wellness']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words =' '.join([text for text in df['Text'][df['category'] == 'Long term Viability of Core Business']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words =' '.join([text for text in df['Text'][df['category'] == 'Training & Development']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words =' '.join([text for text in df['Text'][df['category'] == 'Board Structure & Independence']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normal_words =' '.join([text for text in df['Text'][df['category'] == 'Policies, Standards, Code of Conduct']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Applying N-gram**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df[\"Text\"],df[\"category\"], test_size = 0.25, random_state = 42)    \n",
    "count_vect = CountVectorizer(ngram_range=(1, 2))        \n",
    "transformer = TfidfTransformer(norm='l2',sublinear_tf=True)\n",
    "x_train_counts = count_vect.fit_transform(x_train)\n",
    "x_train_tfidf = transformer.fit_transform(x_train_counts)\n",
    "\n",
    "x_test_counts = count_vect.transform(x_test)\n",
    "x_test_tfidf = transformer.transform(x_test_counts)\n",
    "\n",
    "print (x_train_tfidf.shape,x_test_tfidf.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(count_vect, 'count_vect.pkl')\n",
    "model = joblib.load('count_vect.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning Models**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
    "lr.fit(x_train_tfidf, y_train)\n",
    "y_pred1 = lr.predict(x_test_tfidf)\n",
    "print(\"Accuracy: \"+str(accuracy_score(y_test,y_pred1)))\n",
    "print(classification_report(y_test, y_pred1, zero_division=0))\n",
    "\n",
    "# Precision: Of all instances predicted as positive, 91% were actually positive.\n",
    "# Recall :  Of all actual positive instances, 83% were correctly identified by the model.\n",
    "# f1-score: The harmonic mean of precision and recall, indicating a balance between the two.\n",
    "# Support : The actual occurrences of each class in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "joblib.dump(lr, 'logistic_regression_model.pkl')\n",
    " \n",
    "# Save the CountVectorizer\n",
    "joblib.dump(count_vect, 'count_vect.pkl')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = count_vect.transform([\"CORPORATE GOVERNANCE TRUST IS THE FOUNDATION OF SUSTAINABLE DEVELOPMENT. TRUSTWORTHY RELATIONS BETWEEN THE ORGANISATION AND ITS STAKEHOLDERS IS KEY TO SUSTAINING IN TODAY'S COMPETITIVE BUSINESS ENVIRONMENT. We have promoted and practised the tenets of good corporate governance since inception. Have garnered the trust of our investors by employing funds judiciously, yet competitively, and generating a steady stream of returns. We have reiterated the credibility & capability of our leadership time and again, by looking beyond the bend and acting before the herd. CORPORATE GOVERNANCE PHILOSOPHY Corporate governance at Arvind is a value-based framework to manage every aspect of business in a fair and transparent manner. We use this framework to maintain accountability in all our activities, and employ democratic and open processes. We have evolved guidelines and best practices over the years, to ensure timely and accurate disclosure of information regarding our financials, performance, leadership and governance of the Company. Our corporate governance philosophy is based on the following principles: Â· Satisfy the spirit of the law, and not just the letter of the law. Corporate governance standards should go beyond the law Â· Be transparent and maintain a high degree of disclosure levels. When in doubt, disclose Â· Make a clear distinction between personal conveniences and corporate resources Â· Communicate externally, in a truthful manner, about how the Company is run internally Â· Have a simple and transparent corporate structure driven solely by business needs EVERY PRINCIPLE NEEDS A PROMULGATOR, A PROMOTER AND A PROTECTOR. FOR ARVIND, THESE ROLES ARE ABLY PERFORMED BY OUR BOARD OF DIRECTORS. . The Management is the trustee of the shareholders' capital, and not the owner 11\"])\n",
    "m = transformer.transform(mc)\n",
    "y_pred = lr.predict(m)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Support Vector Machine**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(x_train_tfidf, y_train)\n",
    "y_pred2 = svc.predict(x_test_tfidf)\n",
    "print(\"Accuracy: \"+str(accuracy_score(y_test,y_pred2)))\n",
    "print(classification_report(y_test, y_pred2, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = count_vect.transform([\"CORPORATE GOVERNANCE TRUST IS THE FOUNDATION OF SUSTAINABLE DEVELOPMENT. TRUSTWORTHY RELATIONS BETWEEN THE ORGANISATION AND ITS STAKEHOLDERS IS KEY TO SUSTAINING IN TODAY'S COMPETITIVE BUSINESS ENVIRONMENT. We have promoted and practised the tenets of good corporate governance since inception. Have garnered the trust of our investors by employing funds judiciously, yet competitively, and generating a steady stream of returns. We have reiterated the credibility & capability of our leadership time and again, by looking beyond the bend and acting before the herd. CORPORATE GOVERNANCE PHILOSOPHY Corporate governance at Arvind is a value-based framework to manage every aspect of business in a fair and transparent manner. We use this framework to maintain accountability in all our activities, and employ democratic and open processes. We have evolved guidelines and best practices over the years, to ensure timely and accurate disclosure of information regarding our financials, performance, leadership and governance of the Company. Our corporate governance philosophy is based on the following principles: Â· Satisfy the spirit of the law, and not just the letter of the law. Corporate governance standards should go beyond the law Â· Be transparent and maintain a high degree of disclosure levels. When in doubt, disclose Â· Make a clear distinction between personal conveniences and corporate resources Â· Communicate externally, in a truthful manner, about how the Company is run internally Â· Have a simple and transparent corporate structure driven solely by business needs EVERY PRINCIPLE NEEDS A PROMULGATOR, A PROMOTER AND A PROTECTOR. FOR ARVIND, THESE ROLES ARE ABLY PERFORMED BY OUR BOARD OF DIRECTORS. . The Management is the trustee of the shareholders' capital, and not the owner\"])\n",
    "m = transformer.transform(mc)\n",
    "y_pred = svc.predict(m)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sentiment Scoring**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Calculate TF-IDF weights\n",
    "vectorizer = TfidfVectorizer()   #convert text data into numerical vectors using Term Frequency-Inverse Document Frequency (TF-IDF) \n",
    "tfidf_matrix = vectorizer.fit_transform(df['category'])\n",
    "df['tfidf_avg'] = tfidf_matrix.mean(axis=1)  # Calculates the average TF-IDF score for each document by taking the mean across all terms. This average is stored in the new column df['tfidf_avg'].\n",
    "\n",
    "# Calculate positional weight\n",
    "df['position_weight'] = 1 / (df.index + 1)\n",
    "\n",
    "# Calculate weighted impact score (without scaling)\n",
    "df['weighted_impact'] = df['sentiment_score'] * df['tfidf_avg'] * df['position_weight']\n",
    "\n",
    "# Scale the weighted_impact scores to 0-5 range\n",
    "scaler = MinMaxScaler(feature_range=(0, 5))\n",
    "df['scaled_impact'] = scaler.fit_transform(df[['weighted_impact']])\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "df[['text', 'category', 'scaled_impact']].to_csv('sentiment_analysis_results.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Naive Bayes(Multinomial)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(x_train_tfidf, y_train)\n",
    "y_pred3 = mnb.predict(x_test_tfidf)\n",
    "print(\"Accuracy: \"+str(accuracy_score(y_test,y_pred3)))\n",
    "print(classification_report(y_test, y_pred3,zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = count_vect.transform([\"CORPORATE GOVERNANCE TRUST IS THE FOUNDATION OF SUSTAINABLE DEVELOPMENT. TRUSTWORTHY RELATIONS BETWEEN THE ORGANISATION AND ITS STAKEHOLDERS IS KEY TO SUSTAINING IN TODAY'S COMPETITIVE BUSINESS ENVIRONMENT. We have promoted and practised the tenets of good corporate governance since inception. Have garnered the trust of our investors by employing funds judiciously, yet competitively, and generating a steady stream of returns. We have reiterated the credibility & capability of our leadership time and again, by looking beyond the bend and acting before the herd. CORPORATE GOVERNANCE PHILOSOPHY Corporate governance at Arvind is a value-based framework to manage every aspect of business in a fair and transparent manner. We use this framework to maintain accountability in all our activities, and employ democratic and open processes. We have evolved guidelines and best practices over the years, to ensure timely and accurate disclosure of information regarding our financials, performance, leadership and governance of the Company. Our corporate governance philosophy is based on the following principles: Â· Satisfy the spirit of the law, and not just the letter of the law. Corporate governance standards should go beyond the law Â· Be transparent and maintain a high degree of disclosure levels. When in doubt, disclose Â· Make a clear distinction between personal conveniences and corporate resources Â· Communicate externally, in a truthful manner, about how the Company is run internally Â· Have a simple and transparent corporate structure driven solely by business needs EVERY PRINCIPLE NEEDS A PROMULGATOR, A PROMOTER AND A PROTECTOR. FOR ARVIND, THESE ROLES ARE ABLY PERFORMED BY OUR BOARD OF DIRECTORS. . The Management is the trustee of the shareholders' capital, and not the owner\"])\n",
    "m = transformer.transform(mc)\n",
    "y_pred = mnb.predict(m)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Comparison Between ML Models**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comparison_unibi = pd.DataFrame({'Logistic Regression': [accuracy_score(y_test,y_pred1)*100,f1_score(y_test,y_pred1,average='macro')*100,recall_score(y_test, y_pred1,average='micro')*100,precision_score(y_test, y_pred1,average='micro')*100],\n",
    "                            'SVM':[accuracy_score(y_test,y_pred2)*100,f1_score(y_test,y_pred2,average='macro')*100,recall_score(y_test, y_pred2,average='micro')*100,precision_score(y_test, y_pred2,average='micro')*100],\n",
    "                           'Naive Bayes':[accuracy_score(y_test,y_pred3)*100,f1_score(y_test,y_pred3,average='macro')*100,recall_score(y_test, y_pred3,average='micro')*100,precision_score(y_test, y_pred3,average='micro')*100],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Comparison using uni-bi-gram(1,2)') \n",
    "Comparison_unibi.rename(index={0:'Accuracy',1:'F1_score', 2: 'Recall',3:'Precision'}, inplace=True)\n",
    "Comparison_unibi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment score using Static Precision, Recall, F1-Score, Support value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example precision, recall, f1-score, and support data for each materiality topic\n",
    "data = {\n",
    "    'Topic': [\n",
    "        'Board Structure & Independence',\n",
    "        'Customer Health & Safety',\n",
    "        'Disclosure & Labeling',\n",
    "        'Training & Development',\n",
    "        'Impact from Facilities',\n",
    "        'Product Societal Value',\n",
    "        'Access to Services',\n",
    "        'Environmental Accidents & Remediation',\n",
    "        'Diversity & Equal Opportunity',\n",
    "        'Packaging',\n",
    "        'Biodiversity Impacts'\n",
    "    ],\n",
    "    'Precision': [0.85, 0.78, 0.82, 0.74, 0.80, 0.88, 0.76, 0.70, 0.84, 0.77, 0.79],\n",
    "    'Recall': [0.80, 0.72, 0.75, 0.68, 0.76, 0.82, 0.70, 0.65, 0.78, 0.73, 0.75],\n",
    "    'F1-Score': [0.82, 0.75, 0.78, 0.71, 0.78, 0.85, 0.73, 0.67, 0.81, 0.75, 0.77],\n",
    "    'Support': [120, 100, 110, 90, 95, 130, 85, 80, 115, 105, 98]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a function to calculate sentiment score\n",
    "def calculate_sentiment_score(precision, recall, f1_score, support):\n",
    "    # Normalize metrics to the range 0-1\n",
    "    precision_norm = (precision - 0) / (1 - 0)\n",
    "    recall_norm = (recall - 0) / (1 - 0)\n",
    "    f1_score_norm = (f1_score - 0) / (1 - 0)\n",
    "    \n",
    "    # Combine metrics into a single score (you can adjust weights as needed)\n",
    "    sentiment_score = 0.4 * precision_norm + 0.3 * recall_norm + 0.3 * f1_score_norm\n",
    "    \n",
    "    # Scale sentiment score to 0-5 range\n",
    "    sentiment_score_scaled = sentiment_score * 5\n",
    "    \n",
    "    return sentiment_score_scaled\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "df['Sentiment_Score'] = df.apply(lambda row: calculate_sentiment_score(row['Precision'], row['Recall'], row['F1-Score'], row['Support']), axis=1)\n",
    "\n",
    "# Print the DataFrame with sentiment scores\n",
    "print(df[['Topic', 'Sentiment_Score']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sentiment Score**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to get sentiment scores using TextBlob\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity  # Returns a score from -1 (negative) to 1 (positive)\n",
    "\n",
    "# Apply the function to your text data to get sentiment scores\n",
    "df['sentiment_score'] = df['text'].astype(str).apply(get_sentiment)\n",
    "\n",
    "# Print the data with sentiment score\n",
    "df[['text', 'sentiment_score']]\n",
    "# Save text and sentiment scores to a new CSV file\n",
    "df[['text', 'sentiment_score']].to_csv('sentiment_scores.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Impact Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Calculate TF-IDF weights\n",
    "vectorizer = TfidfVectorizer()   #convert text data into numerical vectors using Term Frequency-Inverse Document Frequency (TF-IDF) \n",
    "tfidf_matrix = vectorizer.fit_transform(df['category'])\n",
    "df['tfidf_avg'] = tfidf_matrix.mean(axis=1)  # Calculates the average TF-IDF score for each document by taking the mean across all terms. This average is stored in the new column df['tfidf_avg'].\n",
    "\n",
    "# Calculate positional weight\n",
    "df['position_weight'] = 1 / (df.index + 1)\n",
    "\n",
    "# Calculate weighted impact score (without scaling)\n",
    "df['weighted_impact'] = df['sentiment_score'] * df['tfidf_avg'] * df['position_weight']\n",
    "\n",
    "# Scale the weighted_impact scores to 0-5 range\n",
    "scaler = MinMaxScaler(feature_range=(0, 5))\n",
    "df['scaled_impact'] = scaler.fit_transform(df[['weighted_impact']])\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "df[['text', 'category', 'scaled_impact']].to_csv('sentiment_analysis_results.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
